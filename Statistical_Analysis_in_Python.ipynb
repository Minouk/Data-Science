{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions in Pandas\n",
    "------\n",
    "\n",
    "- Distribution means: Set of all possible random variables.\n",
    "- Example:\n",
    " - Flipping a **coin** for heads and tails\n",
    " - A **binomial distribution** (two possible outcomes)\n",
    " - Discrete (categories of heads an tails, no real numbers)\n",
    " - Evenle weighted (heads are just as likely as tails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `numpy` actually has some distributions built into it allowing us to make random flips of a coin with given parameters. \n",
    "- Here we ask for a number from the `numpy` binomial distribution. We have two parameters to pass in. The first is the number of times we want it to run. The second is the chance we get a zero, which we will use to represent heads here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.binomial(1, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What if we run the simulation a thousand times and divided the result by a thousand. Well you see a number pretty close to 0.5 which means half of the time we had a heads and half of the time we had a tails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.binomial(1000, 0.5) / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can also have unevenly weighted binomial distributions. For instance what's the chance that there will be a tornado today? It is pretty low. So maybe there is a hundredth of a percentage chance. \n",
    "- We can put this into a binomial distribution as a weighting in NumPy. If we run this 100,000 times we see there are pretty minimal tornado events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chance_of_tornado = 0.01 / 100\n",
    "np.random.binomial(100000, chance_of_tornado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's take one more example. Let's say the chance of a tornado here in Ann Arbor on any given day, is 1% regardless of the time of year. And lets say if there's a tornado I'm going to get away from the windows and hide, then come back and do my recording the next day. So what's the chance of this happening two days in a row?  \n",
    "\n",
    "$~$\n",
    "\n",
    "- Here we create an empty list and we create a number of potential tornado events by asking the NumPy binomial function using our chance of tornado. We'll do this a million times which is just shy of 3,000 years worth of events. \n",
    "- This process is called sampling the distribution. \n",
    "- Now we can write a little loop to go through the list and look for any two adjacent pairs of ones which means that there were two days that had back to back tornadoes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chance_of_tornado = 0.01\n",
    "\n",
    "tornado_events = np.random.binomial(1, chance_of_tornado, 1000000)\n",
    "\n",
    "two_days_in_a_row = 0\n",
    "\n",
    "for j in range(1, len(tornado_events)-1):\n",
    "    if tornado_events[j]==1 and tornado_events[j-1]==1:\n",
    "        two_days_in_a_row += 1\n",
    "        \n",
    "print(f'{two_days_in_a_row} tornadoes back to back in {1000000/365}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many of the distributions you use in data science are not discrete binomial, and instead are continues where the value of the given observation isn't a category like heads or tails, but can be represented by a real number. \n",
    "- It's common to then graph these distributions when talking about them, where the x axis is the value of the observation and the y axis represents the probability that a given observation will occur.  \n",
    "- If all numbers are equally likely to be drawn when you sample from it, this should be graphed as a flat horizontal line. And this flat line is actually called the **uniform distribution**.  \n",
    "$~$\n",
    "\n",
    "- There are few other distributions that get a lot more interesting. Let's take the **normal distribution** which is also called **Gaussian Distribution** or sometimes, a **Bell Curve**. \n",
    "- This distribution looks like a hump where the number which has the highest probability of being drawn is a zero, and there are two decreasing curves on either side of the X axis. \n",
    "- One of the properties of this distribution is that the mean is zero, not the two curves on either side are symmetric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.uniform(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.normal(0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I want to introduce you to the term **expected value**. \n",
    "- I think that most of us are familiar with the mean is the sum of all the values divided by the total number of values. \n",
    "- The **expected value** is the probability from the underlying distribution,it is what would be the mean of a die roll if we did an infinite number of rolls. The result is 3.5 since each face of the die is equally likely to appear. Thus the expected value is 3.5, while the mean value depends upon the samples that we've taken, and converges to the expected value given a sufficiently large sample set.     \n",
    "<br></br>\n",
    "- A second property is  **variance**. Variance is a measure of how broadly values of samples are spread out from the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's get a little bit more formal about five different characteristics of distributions.\n",
    "\n",
    " - First, we can talk about the **distribution central tendency**, and the measures we would use for this are **mode**, **median**, or **mean**. This characteristic is really about where the bulk of probability is in the distribution.\n",
    " - We can also talk about the **variability** in the distribution. The **standard deviation**,and the **interquartile range**. The standard deviation is simply a measure of how different each item, in our sample, is from the mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula for standard deviation\n",
    "$$\\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\overline{x})^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = np.random.normal(0.75,size=1000)\n",
    "\n",
    "np.sqrt(np.sum((np.mean(distribution)-distribution)**2)/len(distribution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-                                                                      \n",
    " - There are a couple of more measures of distribution that are interesting to talk about. One of these is the shape of the tales of the distribution and this is called the **kurtosis**. \n",
    " - We can measure the kurtosis using the statistics functions in the **scipy** package.\n",
    " - A negative value means the curve is slightly more flat than a normal distribution, and a positive value means the curve is slightly more peaky than a normal distribution. \n",
    " - Remember that we aren't measuring the kurtosis of the distribution per se, but of the thousand values which we sampled out of the distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "stats.kurtosis(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We could also move out of the normal distributions and push the peak of the curve one way or the other. And this is called the **skew**. \n",
    "- If we test our current sample data, we see that there isn't much of a skew. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.skew(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's switch distributions and take a look at a distribution called the **Chi Squared** distribution, which is also quite commonly used in statistic. \n",
    "- The **Chi Squared** Distribution has only one parameter called the **degrees of freedom**. \n",
    "- The degrees of freedom is closely related to the number of samples that you take from a normal population. It's important for significance testing.\n",
    "- But what I would like you to observe, is that as the degrees of freedom increases, the shape of the Chi Squared distribution changes. In particular, the skew to the left begins to move towards the center. We can observe this through simulation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First we'll sample 1,000 values from a Chi Squared distribution with degrees of freedom 2. Now we can see that the skew is quite large. Now if we re-sample changing degrees of freedom to 5. We see that the skew has decreased significantly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_squared_df_2 = np.random.chisquare(2, size=10000)\n",
    "stats.skew(chi_squared_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_squared_df_5 = np.random.chisquare(5, size=10000)\n",
    "stats.skew(chi_squared_df_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can actually plot this right in the Jupiter notebook.\n",
    "- You can see a histogram with our plot with the two degrees of freedom is skewed much further to the left, while our plot with the five degrees of freedom is not as highly skewed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "output = plt.hist([chi_squared_df_2, chi_squared_df_5], bins=50, histtype='step',\n",
    "                 label=['2 degrees of freedom', '5 degrees of freedom'])\n",
    "\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \n",
    " - The last aspect of distributions that I want to talk about is the **modality**.\n",
    " - So far, all of the distributions I've shown have a single high point, a peak. But what if we have multiple peaks? \n",
    " - This distribution has two high points, so we call it **bimodal**. These are really interesting distributions and happen regularly in data mining. a useful insight is that we can actually model these using two normal distributions with different parameters. These are called **Gaussian Mixture Models** and are particularly useful when clustering data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----- \n",
    "# hypothesis Testing\n",
    "-----\n",
    "- A hypothesis is a statement that we can test.\n",
    "\n",
    "<br></br>\n",
    "- Let's say that we have an expectation that when a new course is launched on a MOOC platform, the keenest students find out about it and all flock to it. \n",
    "- Thus, we might expect that those students who sign up quite quickly after the course is launched with higher performance than those students who signed up after the MOOC has been around for a while In this example, we have samples from two different groups which we want to compare. The early sign ups and the late sign ups. \n",
    "- When we do hypothesis testing, we hold out that our hypothesis as the alternative and we create a second hypothesis called the null hypothesis, which in this case would be that there is no difference between groups. We then examine the groups to determine whether this null hypothesis is true or not. \n",
    "\n",
    "<br></br>\n",
    " - Alternative hypothesis: Our idea, e.g. there is a difference between groups\n",
    " - Null hypothesis: The alternative of our idea, e.g. there is no difference between groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('grades.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early = df[df['assignment1_submission']<='2015-12-31']\n",
    "later = df[df['assignment1_submission']>'2015-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "later.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When doing hypothesis testing, we have to choose a significance level as a threshold for how much of a chance we're willing to accept. This significance level is typically called **alpha**. It can vary greatly, depending on what you're going to do with the result and the amount of noise you expect in your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For this example, let's use a threshold of 0.05 for our **alpha** or 5%. Now, how do we actually test whether these means are different in Python? The `scipy` library contains a number of different statistical tests and forms a basis for hypothesis testing in Python. \n",
    "- A T test is one way to compare the means of two different populations. In the `scipy` library, the T test end function will compare two independent samples to see if they have different means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stats.ttest_ind?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we want to compare the assignment grades for the first assignment between the two populations, we could generate a T test by passing these two series into the T test in function. \n",
    "- The result is a tuple with a test `statistic` and a `pvalue`. \n",
    "- The `pvalue` here is much larger than our 0.05. So we cannot reject the null hypothesis, which is that the two populations are the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(early['assignment1_grade'], later['assignment1_grade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(early['assignment2_grade'], later['assignment2_grade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(early['assignment3_grade'], later['assignment3_grade'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

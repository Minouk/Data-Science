{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Dataframes\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We used the DataFrame of store purchases from one of our previous lectures, where the index is a list of stores and the columns store purchase data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([{'Name': 'Chris', 'Item Purchased': 'Sponge', 'Cost': 22.50},\n",
    "                  {'Name': 'Kevyn', 'Item Purchased': 'Kitty Litter', 'Cost': 2.50},\n",
    "                  {'Name': 'Filip', 'Item Purchased': 'Spoon', 'Cost': 5.00}],\n",
    "                 index=['Store 1', 'Store 1', 'Store 2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- f we want to add some new column called Date to the DataFrame, we just use the square bracket operator directly on the DataFrame, as long as the column is as long as the rest of the records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = ['December 1', 'January 1', 'mid-May']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we want to add some new field, may be a delivery flag, that's easy too since it's a scalar value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Delivered'] = True\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The problem comes in when we have only a few items to add. In order for this to work, we have to supply pandas the list which is long enough for the DataFrame, so that each row could be populated. This means that we have to input none values ourselves.\n",
    "- If each of our rows has a unique index, then we could just assign the new column identifier to the series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Feedback'] = ['Positive', None, 'Negative']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For instance, if we reset the index in this example so the DataFrame is labeled from 1 through 2, then we create a new series with these labels, we can apply it. \n",
    "- The nice aspect of this approach is that we could just ignore the items that we don't know about, and pandas will put missing values in for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = df.reset_index()\n",
    "adf['Date'] = pd.Series({0: 'December 1', 2: 'mid-May'})\n",
    "adf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now more commonly, we want to join two larger DataFrames together, and this is a bit more complex. \n",
    "- A Venn Diagram is traditionally used to show set membership. For example, the circle on the left is the population of students at a university. The circle on the right is the population of staff at a university. And the overlapping region in the middle are all of those students who are also staff. \n",
    "- We could think of these two populations as indices in separate DataFrames, maybe with the label of Person Name.\n",
    "\n",
    "$~$\n",
    "\n",
    "- When we want to join the DataFrames together, we have some choices to make.\n",
    " - First what if we want a list of all the people regardless of whether they're staff or student, and all of the information we can get on them? In database terminology, this is called a **full outer join** And in set theory, it's called a **union**. In the Venn diagram, it represents everyone in any circle. \n",
    " - It's quite possible though that we only want those people who we have maximum information for, those people who are both staff and students. In database terminology, this is called an **inner join**. Or in set theory, the **intersection**. And this is represented in the Venn diagram as the overlapping parts of each circle. \n",
    " \n",
    "$~$\n",
    "\n",
    "- An exapmle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff_df = pd.DataFrame([{'Name': 'Kelly', 'Role': 'Director of HR'},\n",
    "                         {'Name': 'Sally', 'Role': 'Course liasion'},\n",
    "                         {'Name': 'James', 'Role': 'Grader'}])\n",
    "staff_df = staff_df.set_index('Name')\n",
    "student_df = pd.DataFrame([{'Name': 'James', 'School': 'Business'},\n",
    "                           {'Name': 'Mike', 'School': 'Law'},\n",
    "                           {'Name': 'Sally', 'School': 'Engineering'}])\n",
    "student_df = student_df.set_index('Name')\n",
    "print(staff_df.head())\n",
    "print()\n",
    "print(student_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we want the union of these, we would call merge passing in the DataFrame on the left and the DataFrame on the right, and telling merge that we want it to use an **outer** join. \n",
    "- We tell merge that we want to use the left and right indices as the joining columns. \n",
    "- We can see everyone is listed in this new dataframe, since Mike does not have a role, and John does not have a school, those cells are listed as missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(staff_df, student_df, how='outer', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  If we wanted to get the intersection, that is, just those students who are also staff, we could set the how attribute to **inner**. And we set the resulting DataFrame has only James and Sally in it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(staff_df, student_df, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now there are two other common use cases when merging DataFrames. Both are examples of what we would call set addition. \n",
    " - The first is when we would want to get a list of all staff regardless of whether they were students or not. But if they were students, we would want to get their student details as well. To do this we would use a **left** join. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(staff_df, student_df, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \n",
    " - Next, We want a list of all of the students and their roles if they were also staff. To do this we would do a **right** join. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(staff_df, student_df, how='right', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The merge method has a couple of other interesting parameters. \n",
    " - First, you don't need to use indices to join on, you can use columns as well. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff_df = staff_df.reset_index()\n",
    "student_df = student_df.reset_index()\n",
    "pd.merge(staff_df, student_df, how='left', left_on='Name', right_on='Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So what happens when we have conflicts between the DataFrames? Let's take a look by creating new staff and student DataFrames that have a location information added to them. In the staff DataFrame, this is an office location where we can find the staff person. And we can see the Director of HR is on State Street, while the two students are on Washington Avenue. But for the student DataFrame, the location information is actually their home address. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff_df = pd.DataFrame([{'Name': 'Kelly', 'Role': 'Director of HR', 'Location': 'State Street'},\n",
    "                         {'Name': 'Sally', 'Role': 'Course liasion', 'Location': 'Washington Avenue'},\n",
    "                         {'Name': 'James', 'Role': 'Grader', 'Location': 'Washington Avenue'}])\n",
    "student_df = pd.DataFrame([{'Name': 'James', 'School': 'Business', 'Location': '1024 Billiard Avenue'},\n",
    "                           {'Name': 'Mike', 'School': 'Law', 'Location': 'Fraternity House #22'},\n",
    "                           {'Name': 'Sally', 'School': 'Engineering', 'Location': '512 Wilson Crescent'}])\n",
    "pd.merge(staff_df, student_df, how='left', left_on='Name', right_on='Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The merge function preserves this information, but appends an `_x` or `_y` to help differentiate between which index went with which column of data. The `_x` is always the left DataFrame information, and the `_y` is always the right DataFrame information. you could control the names of `_x` and `_y` with additional parameters if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's talk about multi-indexing and multiple columns. \n",
    "- It's quite possible that the first name for students and staff might overlap, but the last name might not. In this case, we use a list of the multiple columns that should be used to join keys on the `left_on` and `right_on` parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff_df = pd.DataFrame([{'First Name': 'Kelly', 'Last Name': 'Desjardins', 'Role': 'Director of HR'},\n",
    "                         {'First Name': 'Sally', 'Last Name': 'Brooks', 'Role': 'Course liasion'},\n",
    "                         {'First Name': 'James', 'Last Name': 'Wilde', 'Role': 'Grader'}])\n",
    "student_df = pd.DataFrame([{'First Name': 'James', 'Last Name': 'Hammond', 'School': 'Business'},\n",
    "                           {'First Name': 'Mike', 'Last Name': 'Smith', 'School': 'Law'},\n",
    "                           {'First Name': 'Sally', 'Last Name': 'Brooks', 'School': 'Engineering'}])\n",
    "print(staff_df)\n",
    "print()\n",
    "print(student_df)\n",
    "pd.merge(staff_df, student_df, how='inner', left_on=['First Name','Last Name'], right_on=['First Name','Last Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "# Idiomatic Pandas: Making Code Pandorable\n",
    "-------\n",
    "- The best Python solutions to problems are celebrated as Idiomatic Python.\n",
    "- An idiomatic solution is often one which has both high performance and high readability. \n",
    "- let's see a couple of key features of how you can make your code pandorable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first of these is called method chaining. \n",
    "- chain indexing:\n",
    " - `df.loc['Washtenaw']['Total Population']`\n",
    " - This is generally a bad practice, because pandas could return a copy of a view depending upon numpy.\n",
    " - code smell: If you see a `][` you should think carefully about what you are doing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Method chaining though, is a little bit different. The general idea behind method chaining is that every method on an object returns a reference to that object. The beauty of this is that you can condense many different operations on a DataFrame, for instance, into one line or at least one statement of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('census.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here's an example of two pieces of code in pandas using our census data. The first is the pandorable way to write the code with method chaining. \n",
    "- you can see that when we first run a `where` query, then a `dropna`, then a `set_index`, and then a `rename`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.where(df['SUMLEV']==50)\n",
    "    .dropna()\n",
    "    .set_index(['STNAME','CTYNAME'])\n",
    "    .rename(columns={'ESTIMATESBASE2010': 'Estimates Base 2010'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The second example is a more traditional way of writing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['SUMLEV']==50]\n",
    "df.set_index(['STNAME','CTYNAME'], inplace=True)\n",
    "df.rename(columns={'ESTIMATESBASE2010': 'Estimates Base 2010'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here's another pandas idiom. Python has a wonderful function called map, which is sort of a basis for functional programming in the language. When you want to use map in Python, you pass it some function you want called, and some iterable, like a list, that you want the function to be applied to. The results are that the function is called against each item in the list, and there's a resulting list of all of the evaluations of that function. \n",
    "- Pandas has a similar function called applymap. In applymap, you provide some function which should operate on each cell on a DataFrame, and the return set is itself a DataFrame. \n",
    "- Now I think applymap is fine, but I actually rarely use it. Instead, I find myself often wanting to map across all of the rows in a DataFrame. And pandas has a function that I use heavily there, called apply. ( Dr. Brook said! :D )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, we need to write a function which takes in a particular row of data, finds a minimum and maximum values, and returns a new row of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def min_max(row):\n",
    "    data = row[['POPESTIMATE2010',\n",
    "                'POPESTIMATE2011',\n",
    "                'POPESTIMATE2012',\n",
    "                'POPESTIMATE2013',\n",
    "                'POPESTIMATE2014',\n",
    "                'POPESTIMATE2015']]\n",
    "    return pd.Series({'min': np.min(data), 'max': np.max(data)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then we just need to call `apply` on the DataFrame. `apply` takes the function and the axis on which to operate as parameters. Now, we have to be a bit careful, we've talked about axis zero being the rows of the DataFrame in the past. But this parameter is really the **parameter of the index to use**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(min_max, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you're doing this as part of data cleaning your likely to find yourself wanting to add new data to the existing DataFrame. In that case you just take the row values and add in new columns indicating the max and minimum scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def min_max(row):\n",
    "    data = row[['POPESTIMATE2010',\n",
    "                'POPESTIMATE2011',\n",
    "                'POPESTIMATE2012',\n",
    "                'POPESTIMATE2013',\n",
    "                'POPESTIMATE2014',\n",
    "                'POPESTIMATE2015']]\n",
    "    row['max'] = np.max(data)\n",
    "    row['min'] = np.min(data)\n",
    "    return row\n",
    "df.apply(min_max, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = ['POPESTIMATE2010',\n",
    "        'POPESTIMATE2011',\n",
    "        'POPESTIMATE2012',\n",
    "        'POPESTIMATE2013',\n",
    "        'POPESTIMATE2014',\n",
    "        'POPESTIMATE2015']\n",
    "df.apply(lambda x: np.max(x[rows]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "# Group by\n",
    "-------\n",
    "\n",
    "- We've seen that even though PANDAS allows us to iterate over every row in a data frame this is generally a slow way to accomplish a given task and it's not very pandorable. \n",
    "- For instance, if we wanted to write some code to iterate over all the of the states and generate a list of the average census population numbers. We could do so using a loop in the `unique` function.\n",
    "- Another option is to use the dataframe `groupby` function. This function takes some column name or names and splits the dataframe up into chunks based on those names, it returns a dataframe group by object. Which can be iterated upon, and then returns a tuple where the first item is the group condition, and the second item is the data frame reduced by that grouping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('census.csv')\n",
    "df = df[df['SUMLEV']==50]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "for state in df['STNAME'].unique():\n",
    "    avg = np.average(df.where(df['STNAME']==state).dropna()['CENSUS2010POP'])\n",
    "    print('Counties in state ' + state + ' have an average population of ' + str(avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "for group, col in df.groupby('STNAME'):\n",
    "    avg = np.average(col['CENSUS2010POP'])\n",
    "    print('Counties in state ' + group + ' have an average population of ' + str(avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you can actually provide a function to group by as well and use that to segment your data. This is a bit of a fabricated example but lets say that you have a big batch job with lots of processing and you want to work on only a third or so of the states at a given time. \n",
    "- We could create some function which returns a number between zero and two based on the first character of the state name. Then we can tell group by to use this function to split up our data frame. It's important to note that in order to do this you need to set the index of the data frame to be the column that you want to group by first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here's an example. We'll create some new function called `fun` and if the first letter of the parameter is a capital `M` we'll return a 0. If it's a capital `Q` we'll return a 1 and otherwise we'll return a 2. \n",
    "- Then we'll pass this function to the data frame `groupby`, and see that the data frame is segmented by the calculated group number.\n",
    "\n",
    "$~$\n",
    "\n",
    "- This kind of technique, which is sort of a light weight hashing, is commonly used to distribute tasks across multiple workers or cores in a processor, nodes in a supercomputer, or disks in a database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('STNAME')\n",
    "\n",
    "def fun(item):\n",
    "    if item[0]<'M':\n",
    "        return 0\n",
    "    if item[0]<'Q':\n",
    "        return 1\n",
    "    return 2\n",
    "\n",
    "for group, frame in df.groupby(fun):\n",
    "    print('There are ' + str(len(frame)) + ' records in group ' + str(group) + ' for processing.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A common work flow with `groupby` is that you split your data, you apply some function, then you combine the results. This is called **split apply combine** pattern.\n",
    "- we've seen the splitting method, but what about apply? Certainly iterative methods as we've seen can do this, but the `groupby` object also has a method called `agg` which is short for aggregate. This method applies a function to the column or columns of data in the group, and returns the results. \n",
    "- With `agg`, you simply pass in a dictionary of the column names that you're interested in, and the function that you want to apply.\n",
    "- For instance to build a summary data frame for the average populations per state, we could just give `agg` a dictionary with the Census 2010 pop key and the numpy average function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('census.csv')\n",
    "df = df[df['SUMLEV']==50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('STNAME').agg({'CENSUS2010POP': np.average})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You see, when you pass in a dictionary to `agg`, it can be used to either to identify the columns to apply a function on or to name an output column if there's multiple functions to be run. The difference depends on the keys that you pass in from the dictionary and how they're named. \n",
    "- In short, while much of the documentation and examples will talk about a single groupby object, there's really two different objects. The data frame groupby and the series groupby. And these objects behave a little bit differently with aggregate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df.groupby(level=0)['POPESTIMATE2010','POPESTIMATE2011']))\n",
    "print(type(df.groupby(level=0)['POPESTIMATE2010']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('STNAME').groupby(level=0)['CENSUS2010POP'].agg([np.average, np.sum])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can do the same thing with a data frame instead of a series. We set the index to be the state name, we group by the index, and we project two columns. The population estimate in 2010, the population estimate in 2011. \n",
    "- When we call aggregate with two parameters, it builds a nice hierarchical column space and all of our functions are applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.set_index('STNAME').groupby(level=0)['POPESTIMATE2010','POPESTIMATE2011']\n",
    "    .agg([np.average, np.sum]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
